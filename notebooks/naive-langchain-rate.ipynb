{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ee40c5-cb8d-4a29-8037-49389067dfed",
   "metadata": {},
   "source": [
    "---\n",
    "title: Naive Langchain Rating\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .qmd\n",
    "      format_name: quarto\n",
    "      format_version: '1.0'\n",
    "      jupytext_version: 1.17.1\n",
    "  kernelspec:\n",
    "    display_name: nix\n",
    "    language: python\n",
    "    name: nix\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9860041-3c74-4add-b1c6-32a7fac9a09e",
   "metadata": {},
   "source": [
    "Set the `cd` to the project root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8aba360-ea2b-4f42-b7aa-f92f7557e1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kerry/mnt/counter/madrs-ai'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "os.chdir(subprocess\n",
    "         .check_output([\"git\", \"rev-parse\", \"--show-toplevel\"])\n",
    "         .decode('utf-8')\n",
    "         .strip())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d7eab-a0dd-4b7e-b3f7-de3cb18b8061",
   "metadata": {},
   "source": [
    "Defining the graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdabb85-c585-46d5-9ad1-10ad84c0c43a",
   "metadata": {},
   "source": [
    "Invocation. The server here was started on the backend with something like\n",
    "```sh\n",
    "python -m llama_cpp.server \\\n",
    "    --model ./models/mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf \\\n",
    "    --model_alias mistral \\\n",
    "    --host 0.0.0.0 --port 8080 \\\n",
    "    --n_gpu_layers -1 \\\n",
    "    --n_ctx 50000 \\\n",
    "    --chat_format chatml-function-calling\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c1907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def build_rater(llm: BaseChatModel):\n",
    "\n",
    "    MadrsFieldScore = Annotated[\n",
    "        int,\n",
    "        Field(\n",
    "            description=\"The numerical score for this field.\",\n",
    "            ge=0,\n",
    "            le=6,\n",
    "        )\n",
    "    ]\n",
    "    class MadrsFieldRating(BaseModel):\n",
    "        score: int = Field(\n",
    "            description=\"The numerical score for this field.\",\n",
    "            ge=0,\n",
    "            le=6,\n",
    "        )\n",
    "        justification: str = Field(\n",
    "            description=\"A brief justification for the score given for this field.\"\n",
    "        )\n",
    "\n",
    "    class MadrsRating(BaseModel):\n",
    "        apparent_sadness: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for apparent sadness.\"\n",
    "        )\n",
    "        reported_sadness: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for reported sadness.\"\n",
    "        )\n",
    "        inner_tension: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for inner tension.\"\n",
    "        )\n",
    "        reduced_sleep: MadrsFieldRating = Field(\n",
    "            description = \"The MADRSrating for reduced sleep.\"\n",
    "        )\n",
    "        reduced_appetite: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for reduced appetite.\"\n",
    "        )\n",
    "        concentration_difficulties: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for concentration difficulties.\"\n",
    "        )\n",
    "        lassitude: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for lassitude.\"\n",
    "        )\n",
    "        inability_to_feel: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for inability to feel.\"\n",
    "        )\n",
    "        pessimistic_thoughts: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for pessimistic thoughts.\"\n",
    "        )\n",
    "        suicidal_thoughts: MadrsFieldRating = Field(\n",
    "            description = \"The MADRS rating for suicidal thoughts.\"\n",
    "        )\n",
    "\n",
    "    class RaterState(MessagesState):\n",
    "        transcript: str\n",
    "        primer: str\n",
    "        rating: Optional[MadrsRating]\n",
    "\n",
    "\n",
    "    def build_prompt(state: RaterState):\n",
    "        prompt_template = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are an expert in psychiatric assessments and depression. For your\n",
    "            recollection, consider the following primer on MADRS interviews:\n",
    "\n",
    "            ---\n",
    "\n",
    "            {primer}\n",
    "\n",
    "            ---\n",
    "\n",
    "            Using this information, provide the most accurate assessment of the\n",
    "            following transcription of a MADRS interview. Provide your scores for\n",
    "            each of the items described in the primer, providing a best guess for\n",
    "            apparent sadness, together with a brief justification/discussion.\n",
    "\n",
    "            ---\n",
    "\n",
    "            {transcript}\n",
    "            \"\"\"\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": prompt_template.invoke(\n",
    "                {\n",
    "                    \"transcript\": state[\"transcript\"],\n",
    "                    \"primer\": state[\"primer\"]\n",
    "                }\n",
    "            ).to_messages()\n",
    "        }\n",
    "\n",
    "    def invoke_llm(state: RaterState):\n",
    "        return { \"messages\": llm.invoke(state[\"messages\"]) }\n",
    "\n",
    "    def invoke_structured_llm(state: RaterState):\n",
    "        parse_message = HumanMessage(\"Please format your previous response in the prescibed way.\")\n",
    "        return {\n",
    "            # Required to work with llama-cpp server, json_schema not supported yet\n",
    "            \"rating\": llm.with_structured_output(MadrsRating, method=\"function_calling\").invoke(\n",
    "                state[\"messages\"] + [parse_message]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    return (\n",
    "        StateGraph(RaterState)\n",
    "        .add_node(\"build_prompt\", build_prompt)\n",
    "        .add_node(\"invoke_llm\", invoke_llm)\n",
    "        .add_node(\"invoke_structured_llm\", invoke_structured_llm)\n",
    "        .add_edge(START, \"build_prompt\")\n",
    "        .add_edge(\"build_prompt\", \"invoke_llm\")\n",
    "        .add_edge(\"invoke_llm\", \"invoke_structured_llm\")\n",
    "        .add_edge(\"invoke_structured_llm\", END)\n",
    "        .compile()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cabcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"mistral\",\n",
    "    base_url = \"http://localhost:8080/v1\",\n",
    "    api_key = \"NULL\",\n",
    ")\n",
    "transcripts_path = Path(\"data/interviews/transcripts/corrected\")\n",
    "primer_path = Path(\"data/madrs-instructions/primer.md\")\n",
    "transcripts_paths = [transcripts_path / file for file in os.listdir(\"data/interviews/transcripts/corrected\") if file.endswith(\"txt\")]\n",
    "graph = build_rater(llm)\n",
    "with open(primer_path, \"r\") as file:\n",
    "    primer = file.read()\n",
    "results = {}\n",
    "for transcript_path in transcripts_paths:\n",
    "    with open(transcript_path) as file:\n",
    "        transcript = file.read()\n",
    "    response = graph.invoke({\n",
    "        \"transcript\": transcript,\n",
    "        \"primer\" : primer\n",
    "    })\n",
    "    results[transcript_path.name.rstrip(\".txt\")] = {\n",
    "        \"rating\": response[\"rating\"].model_dump(),\n",
    "        \"unstructured_rating\": response[\"messages\"][-1].content\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ce8c0-cf87-40dd-8005-23eef79bcd3c",
   "metadata": {},
   "source": [
    "Push the output to a temp file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1073de8d-8257-43b9-82fa-f563829fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_path = Path(\"temp/output.json\")\n",
    "with output_path.open(mode='w') as file:\n",
    "    file.write(json.dumps(results, indent=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nix",
   "language": "python",
   "name": "nix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
